{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('../utils'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # for feature scaling\n",
    "from sklearn.model_selection import train_test_split  # for train/test split\n",
    "import matplotlib.pyplot as plt  #For representation\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import l1_min_c  # for L1 regluarization path\n",
    "\n",
    "# Example code for calculating accuracy, precision, recall, and F1-score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from statkit.decision import net_benefit\n",
    "from statkit.decision import net_benefit\n",
    "from scipy import integrate\n",
    "\n",
    "\n",
    "import dca_fs_tools as dcat\n",
    "import dca_fs as dcafs\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 1000\n",
    "\n",
    "df_train, df_test, ind_var_names = dcat.make_class_dataset(n_sample = n_sample,\n",
    "                                       n_features = 5,\n",
    "                                       n_redundant = 0,\n",
    "                                       random_state = 1001,\n",
    "                                       n_informative = 4,\n",
    "                                       n_clusters_per_class = 1,\n",
    "                                       n_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=10**18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1']\n",
      "0.21385750755249888\n",
      "['x1', 'x4']\n",
      "0.2244450144712049\n",
      "['x1', 'x4', 'x3']\n",
      "0.18334457454593073\n",
      "['x1', 'x4', 'x3', 'x0']\n",
      "0.1782937341943237\n",
      "['x1', 'x4', 'x3', 'x0', 'x2']\n",
      "0.15340989796623714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit full model\n",
    "#logreg.fit(df_train[ind_var_names], df_train[\"y\"])\n",
    "\n",
    "# define feature harms\n",
    "test_harms = {\"x0\": 0.0, \"x1\": 0.015, \"x2\": 0.03, \"x3\": 0.045, \"x4\": 0.06 }\n",
    " \n",
    "dependent = \"y\"\n",
    "\n",
    "#identify feature imporrtances\n",
    "skl_path = dcat.skl_reg_path(df_train,\n",
    "                            log_space_min = 0,\n",
    "                            log_space_max = 3.5,\n",
    "                            log_space_steps = 64)\n",
    "\n",
    "# order featuures by l1 regularization\n",
    "feature_imp = (skl_path.loc[:, ind_var_names]==0).astype(int).sum(axis=0).sort_values().index.to_list()\n",
    "\n",
    "# itterate through the features fitting a model for each subset\n",
    "# Calculate the mean net benefit for each model select the modle\n",
    "# that maximizes mean net benefit\n",
    "\n",
    "for i in range(len(ind_var_names)):\n",
    "    model_features = feature_imp[0:i+1]\n",
    "    print(model_features)\n",
    "\n",
    "    # fit full model\n",
    "    logreg.fit(df_test[model_features], df_test[\"y\"])\n",
    "\n",
    "    # Make predictions on the test set:\n",
    "    pred = logreg.predict_proba(df_test[model_features])\n",
    "\n",
    "    # auc\n",
    "    #auc = roc_auc_score(df_test[dependent],pred[:, 1])\n",
    "\n",
    "    # mnb\n",
    "    mnb = dcat.mean_net_benefit(df_test[dependent], pred[:, 1], n_thresh=100)['mnb']\n",
    "\n",
    "    #Include test harms\n",
    "    harm = sum([test_harms[i] for i in model_features]) \n",
    "\n",
    "    mnb = mnb - harm\n",
    "    print(mnb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_dcafs(train_data, test_data, model, harms_dict, dependent=\"y\"):\n",
    "\n",
    "    independent = harms_dict.keys()\n",
    "\n",
    "    #identify feature imporrtances with the regularization path (other methods are possible)\n",
    "    skl_path = dcat.skl_reg_path(train_data,\n",
    "                                log_space_min = 0,\n",
    "                                log_space_max = 3.5,\n",
    "                                log_space_steps = 64)\n",
    "\n",
    "    # order featuures by l1 regularization\n",
    "    feature_imp = (skl_path.loc[:, independent]==0).astype(int).sum(axis=0).sort_values().index.to_list()\n",
    "\n",
    "    # itterate through the features fitting a model for each subset\n",
    "    # Calculate the mean net benefit for each model select the modle\n",
    "    # that maximizes mean net benefit\n",
    "    \n",
    "    for i in range(len(independent)):\n",
    "        model_features = feature_imp[0:i+1]\n",
    "        \n",
    "        # fit full model\n",
    "        model.fit(test_data[model_features], test_data[\"y\"])\n",
    "\n",
    "        # Make predictions on the test set:\n",
    "        pred = model.predict_proba(test_data[model_features])\n",
    "\n",
    "        # auc\n",
    "        #auc = roc_auc_score(df_test[dependent],pred[:, 1])\n",
    "\n",
    "        # mnb\n",
    "        mnb = dcat.mean_net_benefit(test_data[dependent], pred[:, 1], n_thresh=100)['mnb']\n",
    "\n",
    "        #Include test harms\n",
    "        harm = sum([test_harms[i] for i in model_features]) \n",
    "\n",
    "        mnb = mnb - harm\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "{\"a\": [1,2,3], \"b\": 6}.keys()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
