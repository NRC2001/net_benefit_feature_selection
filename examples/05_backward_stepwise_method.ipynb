{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('../utils'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # for feature scaling\n",
    "from sklearn.model_selection import train_test_split  # for train/test split\n",
    "import matplotlib.pyplot as plt  #For representation\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import l1_min_c  # for L1 regluarization path\n",
    "\n",
    "# Example code for calculating accuracy, precision, recall, and F1-score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from statkit.decision import net_benefit\n",
    "from statkit.decision import net_benefit\n",
    "from scipy import integrate\n",
    "\n",
    "\n",
    "import dca_fs_tools as dcat\n",
    "import dca_fs as dcafs\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward stepwise selection based on mean net benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a backward stepwise selection procedure based on net benefit.\n",
    "\n",
    "Features are itteratively removed from the full model with the stopping rule that the model with maximum net benefit (or mean net benefit across all threshold probabilities).\n",
    "\n",
    "This example is based on the scikit learn make_classification synthetic data set described in [00_synthetic_data_description.ipynb](./00_synthetic_data_description.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the synthetic data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 1000\n",
    "\n",
    "df_train, df_test, ind_var_names = dcat.make_class_dataset(n_sample = n_sample,\n",
    "                                       n_features = 5,\n",
    "                                       n_redundant = 0,\n",
    "                                       random_state = 1001,\n",
    "                                       n_informative = 4,\n",
    "                                       n_clusters_per_class = 1,\n",
    "                                       n_classes = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the logistic regression model (this could be any type of model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=10**18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for performing the backward stepwise selection procedure. The steps are described in code comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_stepwise_dcafs(train_data, test_data, model, harms_dict, dependent=\"y\"):\n",
    "\n",
    "    features_left = test_harms.keys()\n",
    "\n",
    "    # Initialize with full model\n",
    "    out = pd.DataFrame(columns = [\"features\", \"mnb\"])\n",
    "    model.fit(train_data[features_left], train_data[dependent])\n",
    "    pred = model.predict_proba(test_data[features_left])\n",
    "    mnb = dcat.mean_net_benefit(test_data[dependent], pred[:, 1], n_thresh=100)['mnb']\n",
    "    harm = sum([test_harms[i] for i in features_left]) \n",
    "    mnb = mnb - harm\n",
    "    out.loc[0] = pd.Series({\"features\": list(features_left), \"mnb\": mnb})\n",
    "\n",
    "    for n_dropped in range(len(test_harms.keys())-1):\n",
    "        #print(n_dropped)\n",
    "\n",
    "        mnb_per_drop = pd.DataFrame(columns = [\"droped_feature\", \"model_features\",\"mnb\"])\n",
    "        for i, droped_feature in enumerate(features_left):\n",
    "            #build a model with  the looped feature removed\n",
    "    \n",
    "            model_features = [i for i in features_left if not(i == droped_feature)]\n",
    "\n",
    "            # build a model with the selected features\n",
    "            model.fit(train_data[model_features], train_data[dependent])\n",
    "\n",
    "            ## Make predictions on the test set:\n",
    "            pred = model.predict_proba(test_data[model_features])\n",
    "\n",
    "            # auc\n",
    "            #auc = roc_auc_score(df_test[dependent],pred[:, 1])\n",
    "\n",
    "            # mnb\n",
    "            mnb = dcat.mean_net_benefit(test_data[dependent], pred[:, 1], n_thresh=100)['mnb']\n",
    "\n",
    "            #Include test harms\n",
    "            harm = sum([test_harms[i] for i in model_features]) \n",
    "\n",
    "            mnb = mnb - harm\n",
    "\n",
    "            mnb_per_drop.loc[i] = pd.Series({\"droped_feature\": droped_feature, \"model_features\": model_features, \"mnb\": mnb})\n",
    "\n",
    "\n",
    "        excluded_feature = mnb_per_drop[mnb_per_drop['mnb']==mnb_per_drop['mnb'].max()]\n",
    "    \n",
    "        ef = excluded_feature[\"droped_feature\"].to_list()\n",
    "        features_left = [f for f in features_left if not f in ef]\n",
    "        mnb = excluded_feature[\"mnb\"].to_list()[0]\n",
    "        out.loc[n_dropped+1] = pd.Series({\"features\": features_left, \"mnb\": mnb})\n",
    "\n",
    "\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the selection procedure on the synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>mnb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[x0, x1, x2, x3, x4]</td>\n",
       "      <td>0.142756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[x0, x1, x2, x4]</td>\n",
       "      <td>0.180113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[x0, x1, x4]</td>\n",
       "      <td>0.212192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[x1, x4]</td>\n",
       "      <td>0.219324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[x1]</td>\n",
       "      <td>0.209659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               features       mnb\n",
       "0  [x0, x1, x2, x3, x4]  0.142756\n",
       "1      [x0, x1, x2, x4]  0.180113\n",
       "2          [x0, x1, x4]  0.212192\n",
       "3              [x1, x4]  0.219324\n",
       "4                  [x1]  0.209659"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_harms = {\"x0\": 0.0, \"x1\": 0.015, \"x2\": 0.03, \"x3\": 0.045, \"x4\": 0.06 }\n",
    "\n",
    "backward_selection = backward_stepwise_dcafs(df_train, df_test, logreg, test_harms)\n",
    "\n",
    "# Save for later comparison to other methods\n",
    "\n",
    "with open('../data/backward_selection.pkl', 'wb') as f:\n",
    "    pickle.dump(backward_selection, f)\n",
    "\n",
    "backward_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the maximum net benefit occurs when features $\\{ x1, x4\\}$ are selected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
