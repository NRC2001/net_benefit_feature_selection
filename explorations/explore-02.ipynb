{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # for feature scaling\n",
    "from sklearn.model_selection import train_test_split  # for train/test split\n",
    "\n",
    "import matplotlib.pyplot as plt  #For representation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Netork modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise sklearn LR (used for comparison)\n",
    "\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make synthetic data #\n",
    "#=====================#\n",
    "\n",
    "\n",
    "def make_synth_data(n_features = 3, n_sample = 1000, true_coefs = [1.,2.,3.]):\n",
    "    # initialise\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    ## parameters\n",
    "    #n_features = 3\n",
    "    #n_sample = 100000\n",
    "\n",
    "    ## True coefficients in linear function\n",
    "    #true_coefs = [random.randrange(-5, 5) for i in range(n_features)]\n",
    "    #true_coefs = [3, -2, 1]\n",
    "\n",
    "    # Standardization of variables\n",
    "    mean_vars = [0 for i in range(n_features)]\n",
    "    var_vars = [1 for i in range(n_features)]\n",
    "\n",
    "    # Amount of noise to add to  each variable\n",
    "    noise_scale = [i**2 for i in range(n_features)]\n",
    "    #noise_scale = [0.05*i/(n_features-1)**2 for i in noise_scale]\n",
    "    noise_scale = [0 for i in noise_scale]\n",
    "\n",
    "    # create normal variables\n",
    "    for n in range(n_features):\n",
    "        df[n] =  np.random.normal(loc=mean_vars[n], scale=var_vars[n], size=n_sample)\n",
    "\n",
    "    # create linear combination\n",
    "    df[\"x\"] = 0\n",
    "    for n in range(n_features):\n",
    "        df[\"x\"] +=  true_coefs[n]*df[n]\n",
    "\n",
    "    # Generate outcome\n",
    "\n",
    "    df[\"p\"] = df.apply(lambda row: 1./(1. + math.exp(-row[\"x\"])), axis=1)\n",
    "    df[\"y\"] = df.apply(lambda row: np.random.binomial(1, row[\"p\"] ), axis=1)\n",
    "\n",
    "    # Add noise to each variable\n",
    "    df_noisy = df.copy()\n",
    "\n",
    "    for n in range(n_features):\n",
    "        df_noisy[n] += np.random.normal(loc=0.0, scale=noise_scale[n], size=n_sample)\n",
    "\n",
    "    #df = df.drop([\"x\", \"p\"], axis=1)\n",
    "    df_noisy = df_noisy.drop([\"x\", \"p\"], axis=1)\n",
    "\n",
    "    # rename the indpendent variables\n",
    "    ind_var_rename = dict()\n",
    "    for ind_var_in in range(n_features) :\n",
    "        ind_var_rename[ind_var_in] = \"x\"+f\"{ind_var_in}\"\n",
    "    df = df.rename(columns=ind_var_rename)\n",
    "    df_noisy = df_noisy.rename(columns=ind_var_rename)\n",
    "\n",
    "    ind_var_names = list(ind_var_rename.values())\n",
    "\n",
    "\n",
    "    # Split the data\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "    df_train, df_test, df_noisy_train, df_noisy_test = train_test_split(df, df_noisy, test_size=0.2, random_state=1234)\n",
    "\n",
    "    # Standardize each split\n",
    "\n",
    "    for indep_var in ind_var_names:\n",
    "        df_train[indep_var] = stats.zscore(df_train[indep_var]) \n",
    "        df_test[indep_var] = stats.zscore(df_test[indep_var]) \n",
    "\n",
    "        df_noisy_train[indep_var] = stats.zscore(df_noisy_train[indep_var]) \n",
    "        df_noisy_test[indep_var] = stats.zscore(df_noisy_test[indep_var]) \n",
    "\n",
    "    # scale data\n",
    "    #sc = StandardScaler()\n",
    "    #df_train = sc.fit_transform(df_train)\n",
    "    #X_test = sc.transform(X_test)\n",
    "\n",
    "    return df_train, df_test, df_noisy_train, df_noisy_test, ind_var_names, n_features, true_coefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader, model, criterion, device):                       \n",
    "    #correct = 0                                               \n",
    "    #total = 0                                                 \n",
    "    running_loss = 0.0                                        \n",
    "    model.eval()\n",
    "      \n",
    "    with torch.no_grad():                                     \n",
    "        for i, data in enumerate(loader):                     \n",
    "            inputs, labels = data                             \n",
    "            inputs = inputs.to(device)                        \n",
    "            labels = labels.to(device)                        \n",
    "            \n",
    "            outputs = model(inputs.float())    \n",
    "\n",
    "            labels_hat=outputs.float()\n",
    "\n",
    "            #loss = criterion(labels_hat, labels1.squeeze().type(torch.LongTensor)-1)\n",
    "            loss = criterion(labels_hat.double(), labels.double())\n",
    "\n",
    "            running_loss = running_loss + loss.item()\n",
    "         \n",
    "    return running_loss/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================#\n",
    "#     LR Training       #\n",
    "#=======================#\n",
    "\n",
    "\n",
    "def lr_train(train_in, n_epochs=50, batch_size = 30, learn_rate = 0.01, save_name_mod = \"\"):\n",
    "    \"\"\"Trains the model.\n",
    "    Args:\n",
    "        csv_file (str): Absolute path of the dataset used for training.\n",
    "        n_epochs (int): Number of epochs to train.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    train_dataset_orig = SynthDataset(train_in)\n",
    "\n",
    "    train_size = int(0.8 * len(train_dataset_orig))\n",
    "    valid_size = len(train_dataset_orig) - train_size\n",
    "    train_dataset, valid_dataset = random_split(train_dataset_orig, [train_size, valid_size])\n",
    "\n",
    "    \n",
    "    # Dataloaders\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Use gpu if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "    # Define the model\n",
    "    net=LogisticRegression(train_dataset_orig.n_features)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Optimizer\n",
    "    #optimizer = optim.Adam(net.parameters(), lr=learn_rate, weight_decay=w_decay)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learn_rate)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "    # Train the net\n",
    "    loss_per_iter = []\n",
    "    loss_per_batch = []\n",
    "\n",
    "    #valid_loss_per_iter = []\n",
    "    valid_loss_per_batch = []\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, label) in enumerate(trainloader):\n",
    "            inputs = inputs.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "\n",
    "            label_hat=outputs.float()\n",
    "\n",
    "            print(label_hat)\n",
    "            print(label)\n",
    "\n",
    "            # calculate loss\n",
    "            #loss=criterion(label_hat, label.squeeze().type(torch.LongTensor)-1)\n",
    "            loss=criterion(label_hat.double(), label.double() )\n",
    "\n",
    "            #loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save loss to plot\n",
    "            running_loss += loss.item()\n",
    "            loss_per_iter.append(loss.item())\n",
    "\n",
    "        loss_per_batch.append(running_loss / (i + 1))\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Evaluate validation loss\n",
    "        vloss = validate(validloader, net, criterion, device)\n",
    "        valid_loss_per_batch.append(vloss)\n",
    "\n",
    "        # If validation loss is lower than lowest then save the model\n",
    "        #if vloss < best_loss:\n",
    "        #    save_network(net, \"best_model\"+save_name_mod)\n",
    "        #    best_loss = vloss\n",
    "\n",
    "  \n",
    "    print(\"Root mean squared error\")\n",
    "    print(\"Training:\", np.sqrt(loss_per_batch[-1]))\n",
    "\n",
    "    return{\"loss_per_batch\": loss_per_batch,\n",
    "           \"valid_loss_per_batch\": valid_loss_per_batch}\n",
    "\n",
    "    # Plot training loss curve\n",
    "##    plt.plot(np.arange(len(loss_per_iter)), loss_per_iter, \"-\", alpha=0.5, label=\"Loss per epoch\")\n",
    "#    plt.plot(np.arange(len(loss_per_batch)), loss_per_batch, \"-\", alpha=0.5, label=\"Loss per epoch\")\n",
    "#    plt.plot(np.arange(len(valid_loss_per_batch)), valid_loss_per_batch, \"-\", alpha=0.5, label=\"Validation Loss per epoch\")\n",
    "#    #plt.plot(np.arange(len(loss_per_iter), step=4) + 3, loss_per_batch, \".-\", label=\"Loss per mini-batch\")\n",
    "#    plt.xlabel(\"Number of epochs\")\n",
    "#    plt.ylabel(\"Loss\")\n",
    "#    plt.legend()\n",
    "#    plt.show()\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch LR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#==================#\n",
    "#    Load Data     #\n",
    "#==================#\n",
    "\n",
    "\n",
    "class SynthDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initializes instance of class SynthDataset.\n",
    "\n",
    "        Args:\n",
    "            df: Pandas dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "#sc = StandardScaler()\n",
    "#df_train = sc.fit_transform(df_train)\n",
    "\n",
    "        # The target is binary\n",
    "        self.target = [\"y\"]   #[\"flow\", \"handling\", \"respect\"]\n",
    "        self.features = ind_var_names\n",
    "        self.data_frame = df.loc[:, self.target+self.features]\n",
    "        self.n_features = len(self.features)\n",
    "\n",
    "        # Save target and predictors\n",
    "        self.x = self.data_frame[self.features]\n",
    "        self.y = self.data_frame.loc[:,\"y\"]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#        return [self.X.iloc[idx].values, self.y[idx]]\n",
    "#        return  {\"X\": self.X.iloc[idx].values, 'flow': self.flow.iloc[idx].values, 'handling': self.handling.iloc[idx].values, 'respect': self.respect.iloc[idx].values}\n",
    "        return  [self.x.iloc[idx].values.astype(float),  self.y.iloc[idx].astype(float)] #,  self.handling.iloc[idx].astype(float),  self.respect.iloc[idx].astype(float)]\n",
    "\n",
    "#test = SynthDataset(df_noisy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         y_predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x))\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m y_predicted\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m TorchLogisticRegression(\u001b[43mn_features\u001b[49m)\n\u001b[1;32m     15\u001b[0m [p[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class TorchLogisticRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input_features):\n",
    "        super(TorchLogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = TorchLogisticRegression(n_features)\n",
    "\n",
    "[p[1] for p in enumerate(model.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4460], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t = torch.from_numpy(np.array([[1,2,1]]))\n",
    "\n",
    "#s = [p[1] for p in enumerate(model.parameters())][0].abs()\n",
    "\n",
    "#(torch.mul(t,s), s)\n",
    "\n",
    "[p[1] for p in enumerate(model.parameters())][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " [p[0] for p in enumerate(model.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class CustomLoss(nn.Module):\n",
    "#    def __init__(self, weight):\n",
    "#        super(CustomLoss, self).__init__()\n",
    "#        self.weight = weight\n",
    "#\n",
    "#    def forward(self, input, target):\n",
    "#        # Compute the loss\n",
    "#        loss = torch.mean(self.weight * (input - target) ** 2)\n",
    "#        return loss\n",
    "    \n",
    "\n",
    "class mnbLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mnbLoss, self).__init__()\n",
    "        #self.weight = weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Compute the loss\n",
    "        #loss = torch.mean(self.weight * (input - target) ** 2)\n",
    "\n",
    "        loss = -torch.mean(target*input+(1-target)*(input*torch.log(1-input)))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "test = mnbLoss()\n",
    "\n",
    "input_tensor = torch.randn(3, requires_grad=True)\n",
    "target_tensor = torch.randn(3)\n",
    "\n",
    "test_loss = test(input_tensor,target_tensor )\n",
    "\n",
    "test_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================#\n",
    "#     LR Training       #\n",
    "#=======================#\n",
    "\n",
    "\n",
    "def lr_train(train_in, n_epochs=100, learn_rate = 0.01, save_name_mod = \"\", loss_fun = 'log'):\n",
    "\n",
    "    # Load dataset\n",
    "    train_dataset_orig = SynthDataset(train_in)\n",
    "\n",
    "    train_x = torch.from_numpy(train_dataset_orig.x.to_numpy().astype(np.float32) )\n",
    "    train_y = torch.from_numpy(train_dataset_orig.y.to_numpy().astype(np.float32))\n",
    "\n",
    "    train_y = train_y.view(train_y.shape[0], 1)\n",
    "\n",
    "    # Define the model\n",
    "    net=TorchLogisticRegression(train_dataset_orig.n_features)\n",
    "    \n",
    "    # Loss function\n",
    "\n",
    "    if loss_fun == 'log':\n",
    "        criterion = nn.BCELoss()\n",
    "    elif loss_fun == 'mnb':\n",
    "        criterion = mnbLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    #optimizer = optim.Adam(net.parameters(), lr=learn_rate, weight_decay=w_decay)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learn_rate)\n",
    "    #torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        y_pred = net(train_x )\n",
    "\n",
    "        loss=criterion(y_pred, train_y )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')        \n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================#\n",
    "#     Regularized LR Training       #\n",
    "#===================================#\n",
    "\n",
    "\n",
    "def reg_lr_train(train_in, n_epochs=100, learn_rate = 0.01, save_name_mod = \"\", loss_fun = 'log', regularization_type = 'L1', weights = []):\n",
    "\n",
    "    # Load dataset\n",
    "    train_dataset_orig = SynthDataset(train_in)\n",
    "\n",
    "    train_x = torch.from_numpy(train_dataset_orig.x.to_numpy().astype(np.float32) )\n",
    "    train_y = torch.from_numpy(train_dataset_orig.y.to_numpy().astype(np.float32))\n",
    "\n",
    "    train_y = train_y.view(train_y.shape[0], 1)\n",
    "\n",
    "    # Define the model\n",
    "    net=TorchLogisticRegression(train_dataset_orig.n_features)\n",
    "    \n",
    "    # Loss function\n",
    "\n",
    "    if loss_fun == 'log':\n",
    "        criterion = nn.BCELoss()\n",
    "    elif loss_fun == 'mnb':\n",
    "        criterion = mnbLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    #optimizer = optim.Adam(net.parameters(), lr=learn_rate, weight_decay=w_decay)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learn_rate)\n",
    "    #torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "\n",
    "    #t = torch.from_numpy(np.array([[1,2,1]]))\n",
    "    #\n",
    "    #s = [p[1] for p in enumerate(model.parameters())][0].abs()\n",
    "\n",
    "    # A tensor of weights for the weighted L1\n",
    "    if regularization_type == 'weighted_L1':\n",
    "        weights = [\n",
    "            torch.from_numpy(np.array([weights])),\n",
    "            torch.from_numpy(np.array([1.0])),\n",
    "        ]\n",
    "\n",
    "    lambda_reg = 0.01\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        y_pred = net(train_x )\n",
    "\n",
    "        loss=criterion(y_pred, train_y )\n",
    "\n",
    "        # add regularization term #\n",
    "        #-------------------------#\n",
    "\n",
    "        # Apply L1 regularization\n",
    "        if regularization_type == 'L1':\n",
    "            l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "            loss += lambda_reg * l1_norm\n",
    "            \n",
    "        # Apply weighted L1 regularization\n",
    "        elif regularization_type == 'weighted_L1':\n",
    "            wl1_norm = sum(torch.mul(weights[p[0]], p[1]).abs().sum() for p in enumerate(net.parameters()))\n",
    "            loss += lambda_reg * wl1_norm\n",
    "\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')        \n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Torch LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agreement of LR parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset\n",
    "\n",
    "df_train, df_test, df_noisy_train, df_noisy_test, ind_var_names, n_features, true_coefs = make_synth_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize synthetic data set\n",
    "\n",
    "x1 = df_train.loc[df_train.y==0]['x0']\n",
    "x2 = df_train.loc[df_train.y==1]['x0']\n",
    "\n",
    "kwargs = dict(alpha=0.5, bins=100)\n",
    "\n",
    "plt.hist(x1, **kwargs, label='0')\n",
    "plt.hist(x2, **kwargs, label='1')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plt.gca().set(title='Frequency Histogram of Diamond Depths', ylabel='Frequency')\n",
    "#plt.xlim(50,75)\n",
    "#plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83969496 1.90189148 2.83292748]]\n"
     ]
    }
   ],
   "source": [
    "# Fit a logistic regression with sklearn\n",
    "\n",
    "logreg.fit(df_train[ind_var_names], df_train[\"y\"])\n",
    "\n",
    "print(logreg.coef_)\n",
    "#print(logreg.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = -0.0508\n",
      "epoch: 20, loss = -0.0880\n",
      "epoch: 30, loss = -0.1146\n",
      "epoch: 40, loss = -0.1339\n",
      "epoch: 50, loss = -0.1502\n",
      "epoch: 60, loss = -0.1641\n",
      "epoch: 70, loss = -0.1760\n",
      "epoch: 80, loss = -0.1861\n",
      "epoch: 90, loss = -0.1948\n",
      "epoch: 100, loss = -0.2021\n",
      "epoch: 110, loss = -0.2083\n",
      "epoch: 120, loss = -0.2136\n",
      "epoch: 130, loss = -0.2181\n",
      "epoch: 140, loss = -0.2220\n",
      "epoch: 150, loss = -0.2254\n",
      "epoch: 160, loss = -0.2282\n",
      "epoch: 170, loss = -0.2308\n",
      "epoch: 180, loss = -0.2330\n",
      "epoch: 190, loss = -0.2349\n",
      "epoch: 200, loss = -0.2366\n",
      "epoch: 210, loss = -0.2381\n",
      "epoch: 220, loss = -0.2394\n",
      "epoch: 230, loss = -0.2406\n",
      "epoch: 240, loss = -0.2417\n",
      "epoch: 250, loss = -0.2427\n",
      "epoch: 260, loss = -0.2436\n",
      "epoch: 270, loss = -0.2444\n",
      "epoch: 280, loss = -0.2451\n",
      "epoch: 290, loss = -0.2458\n",
      "epoch: 300, loss = -0.2464\n",
      "epoch: 310, loss = -0.2469\n",
      "epoch: 320, loss = -0.2475\n",
      "epoch: 330, loss = -0.2479\n",
      "epoch: 340, loss = -0.2484\n",
      "epoch: 350, loss = -0.2488\n",
      "epoch: 360, loss = -0.2491\n",
      "epoch: 370, loss = -0.2495\n",
      "epoch: 380, loss = -0.2498\n",
      "epoch: 390, loss = -0.2501\n",
      "epoch: 400, loss = -0.2504\n",
      "epoch: 410, loss = -0.2507\n",
      "epoch: 420, loss = -0.2509\n",
      "epoch: 430, loss = -0.2511\n",
      "epoch: 440, loss = -0.2513\n",
      "epoch: 450, loss = -0.2515\n",
      "epoch: 460, loss = -0.2517\n",
      "epoch: 470, loss = -0.2519\n",
      "epoch: 480, loss = -0.2521\n",
      "epoch: 490, loss = -0.2522\n",
      "epoch: 500, loss = -0.2524\n",
      "epoch: 510, loss = -0.2525\n",
      "epoch: 520, loss = -0.2526\n",
      "epoch: 530, loss = -0.2528\n",
      "epoch: 540, loss = -0.2529\n",
      "epoch: 550, loss = -0.2530\n",
      "epoch: 560, loss = -0.2531\n",
      "epoch: 570, loss = -0.2532\n",
      "epoch: 580, loss = -0.2533\n",
      "epoch: 590, loss = -0.2534\n",
      "epoch: 600, loss = -0.2535\n",
      "epoch: 610, loss = -0.2535\n",
      "epoch: 620, loss = -0.2536\n",
      "epoch: 630, loss = -0.2537\n",
      "epoch: 640, loss = -0.2538\n",
      "epoch: 650, loss = -0.2538\n",
      "epoch: 660, loss = -0.2539\n",
      "epoch: 670, loss = -0.2539\n",
      "epoch: 680, loss = -0.2540\n",
      "epoch: 690, loss = -0.2540\n",
      "epoch: 700, loss = -0.2541\n",
      "epoch: 710, loss = -0.2541\n",
      "epoch: 720, loss = -0.2542\n",
      "epoch: 730, loss = -0.2542\n",
      "epoch: 740, loss = -0.2543\n",
      "epoch: 750, loss = -0.2543\n",
      "epoch: 760, loss = -0.2543\n",
      "epoch: 770, loss = -0.2544\n",
      "epoch: 780, loss = -0.2544\n",
      "epoch: 790, loss = -0.2544\n",
      "epoch: 800, loss = -0.2545\n",
      "epoch: 810, loss = -0.2545\n",
      "epoch: 820, loss = -0.2545\n",
      "epoch: 830, loss = -0.2546\n",
      "epoch: 840, loss = -0.2546\n",
      "epoch: 850, loss = -0.2546\n",
      "epoch: 860, loss = -0.2546\n",
      "epoch: 870, loss = -0.2547\n",
      "epoch: 880, loss = -0.2547\n",
      "epoch: 890, loss = -0.2547\n",
      "epoch: 900, loss = -0.2547\n",
      "epoch: 910, loss = -0.2547\n",
      "epoch: 920, loss = -0.2548\n",
      "epoch: 930, loss = -0.2548\n",
      "epoch: 940, loss = -0.2548\n",
      "epoch: 950, loss = -0.2548\n",
      "epoch: 960, loss = -0.2548\n",
      "epoch: 970, loss = -0.2548\n",
      "epoch: 980, loss = -0.2548\n",
      "epoch: 990, loss = -0.2549\n",
      "epoch: 1000, loss = -0.2549\n",
      "(800, 6)\n"
     ]
    }
   ],
   "source": [
    "test_run = reg_lr_train(df_train, n_epochs=1000, learn_rate = 0.1, save_name_mod = \"\", loss_fun = 'mnb')\n",
    "#test_run = reg_lr_train(df_train, n_epochs=1000, learn_rate = 0.1, save_name_mod = \"\", loss_fun = 'mnb', regularization_type = 'weighted_L1', weights = [1.0, 1.0, 1.0])\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.5100, 1.3379, 2.0819]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2158], requires_grad=True),\n",
       " [1.0, 2.0, 3.0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_run.linear.weight, test_run.linear.bias, true_coefs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
